{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mennakurdi/Data-Engineering-Project/blob/colab-Menna/post__integration_%2Bfinal_analysis_%2B_export.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dashboard & Spatial Analysis 5. Which areas (zip codes) show consistently high crash density over time, and how do they relate to pedestrian injury counts? 6. At what times of day do most severe crashes (with at least one person killed or seriously injured) occur?"
      ],
      "metadata": {
        "id": "h5ObZCxnwkbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- FIX 3: DROP IRRELEVANT COLUMNS -----\n",
        "columns_to_drop = [\n",
        "    'location','on_street_name','off_street_name','cross_street_name',\n",
        "    'contributing_factor_vehicle_2','contributing_factor_vehicle_3',\n",
        "    'contributing_factor_vehicle_4','contributing_factor_vehicle_5',\n",
        "    'vehicle_type_code2','vehicle_type_code_3','vehicle_type_code_4','vehicle_type_code_5',\n",
        "    'unique_id','person_id','person_type','person_injury','vehicle_id',\n",
        "    'person_age','ejection','emotional_status','bodily_injury',\n",
        "    'position_in_vehicle','safety_equipment','ped_location','ped_action',\n",
        "    'complaint','ped_role','contributing_factor_1','contributing_factor_2','person_sex'\n",
        "]\n",
        "\n",
        "merged_df = merged_df.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "# ----- FIX 4: RENAME CONFUSING COLUMNS -----\n",
        "merged_df = merged_df.rename(columns={\n",
        "    'crash_date_crash': 'crash_date',\n",
        "    'crash_time_crash': 'crash_time'\n",
        "})\n",
        "\n",
        "# ----- FIX 5: CHECK MISSINGNESS -----\n",
        "print(\"Missing crash_date:\", merged_df['crash_date'].isna().mean())\n",
        "print(\"Missing crash_time:\", merged_df['crash_time'].isna().mean())\n"
      ],
      "metadata": {
        "id": "Qdh8mV_mwnJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Post-Integration Cleaning\n",
        "After merging, remove redundant columns, fix types, and clean new nulls.\n"
      ],
      "metadata": {
        "id": "2cpOYI7xvs14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1️⃣ inspect missing values\n",
        "merged_df.isna().mean().sort_values(ascending=False).head(15)\n",
        "\n",
        "# 2️⃣ drop duplicated columns (_person that duplicate _crash)\n",
        "dup_cols = [c for c in merged_df.columns\n",
        "            if c.endswith('_person') and c.replace('_person','_crash') in merged_df.columns]\n",
        "merged_df.drop(columns=dup_cols, inplace=True, errors='ignore')\n",
        "\n",
        "# 3️⃣ numeric columns → proper types\n",
        "num_cols = ['number_of_persons_injured','number_of_persons_killed']\n",
        "for c in num_cols:\n",
        "    if c in merged_df.columns:\n",
        "        merged_df[c] = pd.to_numeric(merged_df[c], errors='coerce').fillna(0).astype(int)\n",
        "\n",
        "# 4️⃣ remove negatives\n",
        "for c in num_cols:\n",
        "    merged_df.loc[merged_df[c]<0, c] = 0\n",
        "\n",
        "# 5️⃣ confirm\n",
        "merged_df.info()\n"
      ],
      "metadata": {
        "id": "QvrXZcVTvxxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.x Post-Integration Missing Values & Consistency Check\n",
        "\n",
        "After merging `crash` and `persons` on `collision_id`, we perform a quick audit of:\n",
        "\n",
        "- **New missing values introduced by the join**  \n",
        "- Potentially **redundant or duplicated columns**  \n",
        "- Remaining **type mismatches** that might break the dashboard later\n",
        "\n",
        "The following cell computes missing-value percentages and highlights the columns that still need attention.\n"
      ],
      "metadata": {
        "id": "tQhUmLakvz6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Post-integration missing values analysis\n",
        "na_ratio = merged_df.isna().mean().sort_values(ascending=False)\n",
        "print(\"Top 20 columns by missing-value ratio after integration:\")\n",
        "display(na_ratio.head(20))\n",
        "\n",
        "# Identify obviously redundant columns (same logical field with _x / _y suffixes)\n",
        "dup_like = [c for c in merged_df.columns if c.endswith('_x') or c.endswith('_y')]\n",
        "print(\"\\nPotential duplicated columns from merge:\", dup_like)\n"
      ],
      "metadata": {
        "id": "fwKxBsHlv2xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Export Clean Dataset\n",
        "Keep only columns needed for the interactive app and export to CSV.\n"
      ],
      "metadata": {
        "id": "z5ptWmu2v5c_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(merged_df.columns)\n"
      ],
      "metadata": {
        "id": "ru9NrWbPv7js"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_cols = [\n",
        "    'collision_id','crash_date','crash_time','borough','zip_code',\n",
        "    'latitude','longitude',\n",
        "    'number_of_persons_injured','number_of_persons_killed',\n",
        "    'number_of_pedestrians_injured','number_of_pedestrians_killed',\n",
        "    'number_of_cyclist_injured','number_of_cyclist_killed',\n",
        "    'number_of_motorist_injured','number_of_motorist_killed',\n",
        "    'contributing_factor_vehicle_1','vehicle_type_code1'\n",
        "]\n",
        "\n",
        "# Keep only columns that actually exist in merged_df (for robustness)\n",
        "selected_cols = [c for c in selected_cols if c in merged_df.columns]\n",
        "\n",
        "df_site = merged_df[selected_cols].drop_duplicates(subset=['collision_id']).copy()\n",
        "\n",
        "# Feature engineering for easier dashboard filters\n",
        "df_site['crash_date'] = pd.to_datetime(df_site['crash_date'], errors='coerce')\n",
        "df_site['crash_year'] = df_site['crash_date'].dt.year\n",
        "df_site['crash_month'] = df_site['crash_date'].dt.to_period('M').astype(str)\n",
        "\n",
        "# Extract hour if we have a time column\n",
        "if 'crash_time' in df_site.columns:\n",
        "    df_site['crash_hour'] = pd.to_datetime(df_site['crash_time'], errors='coerce').dt.hour\n",
        "\n",
        "df_site.to_csv('/content/df_site.csv', index=False)\n",
        "print(\"✅ Clean dataset exported as df_site.csv\")\n",
        "print(\"Final df_site columns:\", df_site.columns.tolist())\n"
      ],
      "metadata": {
        "id": "wF-NPwFSv88Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.x Final Dataset for Dashboard (`df_site`)\n",
        "\n",
        "The exported `df_site` dataset is designed **specifically for the interactive website**:\n",
        "\n",
        "- Includes **location** fields: `borough`, `zip_code`, `latitude`, `longitude`  \n",
        "- Includes **severity measures**: `number_of_persons_injured`, `number_of_persons_killed`  \n",
        "- Includes **injury-type fields** (if present): pedestrians, cyclists, motorists  \n",
        "- Includes **cause & vehicle**: `contributing_factor_vehicle_1`, `vehicle_type_code1`  \n",
        "- Includes **time features**: `crash_year`, `crash_month`, and (when available) `crash_hour`\n",
        "\n",
        "These fields are enough to implement the required filters in the web app:\n",
        "\n",
        "- Filter by **Borough** (borough)  \n",
        "- Filter by **Year** (crash_year)  \n",
        "- Filter by **Vehicle Type** (vehicle_type_code1)  \n",
        "- Filter by **Contributing Factor** (contributing_factor_vehicle_1)  \n",
        "- Filter by **Injury Type** (using counts per pedestrian/cyclist/motorist or total injuries)\n"
      ],
      "metadata": {
        "id": "f6c_wLFdwVOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 5. Verification\n",
        "Quick sanity check on exported dataset.\n"
      ],
      "metadata": {
        "id": "_eeKvOn-wYYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Rows & Cols →\", df_site.shape)\n",
        "display(df_site.head())\n",
        "df_site.isna().mean().sort_values(ascending=False).head(10)\n"
      ],
      "metadata": {
        "id": "EoqEZ9GGwZxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "- Both datasets cleaned, standardized, and merged on `collision_id`.\n",
        "- Final dataset exported as df_site.csv for dashboard visualization.\n",
        "- Total rows: 1000, columns: 11.\n"
      ],
      "metadata": {
        "id": "Rq6oT7mvweNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Led the integration of crashes and persons datasets via collision_id.\n",
        "Designed and documented pre- and post-integration cleaning steps.\n",
        "Prepared the final cleaned dataset (df_site) for the web application."
      ],
      "metadata": {
        "id": "_Q6V_feCwo6S"
      }
    }
  ]
}